# Servicio Local GPU de TranscripciÃ³n ğŸš€

Este documento te guÃ­a para configurar el servicio local de transcripciÃ³n con tu RTX 4070 Super para obtener velocidades de transcripciÃ³n ~50-100x mÃ¡s rÃ¡pidas que OpenAI.

## ğŸ› ï¸ InstalaciÃ³n RÃ¡pida

### 1. Instalar Dependencias Python

```bash
# Instalar PyTorch con CUDA 11.8 (compatible con RTX 4070 Super)
pip install faster-whisper torch torchaudio flask flask-cors --index-url https://download.pytorch.org/whl/cu118
```

### 2. Iniciar Servicio Local

```bash
# En una terminal separada, ejecutar:
python transcription_service.py
```

DeberÃ­as ver algo como:
```
âœ… Faster-Whisper importado correctamente
âœ… PyTorch 2.1.0 disponible
ğŸ® CUDA disponible: True
ğŸš€ GPU: NVIDIA GeForce RTX 4070 Super
ğŸ’¾ VRAM disponible: 12.0 GB
ğŸ“¥ Descargando modelo Whisper 'base' para cuda...
âœ… Modelo Whisper 'base' cargado en cuda
âš¡ Servicio listo para transcripciÃ³n super rÃ¡pida!
ğŸ“¡ Servidor iniciando en http://localhost:8889
```

### 3. Usar en la AplicaciÃ³n

1. Abre tu aplicaciÃ³n web
2. Ve al componente de TranscripciÃ³n HÃ­brida  
3. Cambia de "OpenAI Cloud" a "GPU Local âœ…"
4. Â¡Listo! Ahora tienes transcripciÃ³n ultra rÃ¡pida y gratuita

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Modelos Disponibles

Cambia el modelo en `transcription_service.py` lÃ­nea 53:

- `tiny` - Muy rÃ¡pido, menor calidad
- `base` - Balance Ã³ptimo (recomendado)
- `small` - Mejor calidad, un poco mÃ¡s lento
- `medium` - Excelente calidad, mÃ¡s lento
- `large-v2` o `large-v3` - MÃ¡xima calidad, el mÃ¡s lento

### OptimizaciÃ³n GPU

Para RTX 4070 Super (12GB VRAM):
- Modelo `base`: ~50ms por chunk de audio
- Modelo `small`: ~100ms por chunk de audio  
- Modelo `medium`: ~200ms por chunk de audio

## ğŸ†š ComparaciÃ³n vs OpenAI

| CaracterÃ­stica | OpenAI Cloud | GPU Local |
|----------------|--------------|-----------|
| **Velocidad** | ~2-5 segundos | ~50-200ms |
| **Costo** | $0.006/minuto | $0 (gratis) |
| **Privacidad** | EnvÃ­a audio a OpenAI | 100% local |
| **LÃ­mites** | Rate limits | Sin lÃ­mites |
| **Disponibilidad** | Requiere internet | Funciona offline |

## ğŸ› SoluciÃ³n de Problemas

### Error: CUDA no disponible
```bash
# Verificar instalaciÃ³n CUDA
python -c "import torch; print(torch.cuda.is_available())"
```

### Error: No se puede conectar al servicio
- Verifica que el servicio estÃ© corriendo en http://localhost:8889
- Revisa que no haya otro proceso usando el puerto 8889

### Error: Memoria insuficiente
- Usa un modelo mÃ¡s pequeÃ±o (`tiny` o `base`)
- Cierra otras aplicaciones que usen GPU

## âš¡ Tips de Rendimiento

1. **MantÃ©n el servicio corriendo**: No lo reinicies innecesariamente
2. **Usa modelo base**: Es el mejor balance velocidad/calidad
3. **Monitoring GPU**: Usa `nvidia-smi` para monitorear uso

## ğŸ”„ Actualizaciones

Para actualizar el servicio:
```bash
pip install --upgrade faster-whisper torch torchaudio
```

---

Â¡Disfruta de transcripciÃ³n ultrarrÃ¡pida con tu RTX 4070 Super! ğŸ®âš¡